
@article{karouzos-etal-2021-udalm,
    title = {{UDALM}: Unsupervised Domain Adaptation through Language Modeling},
    selected={true},
    author = {Karouzos, Constantinos  and
      Paraskevopoulos, Georgios  and
      Potamianos, Alexandros},
    booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    month = {6},
    year = {2021},
    address = {Online},
    publisher = {Association for Computational Linguistics},
    abbr = {NAACL 2021},
    url = {https://aclanthology.org/2021.naacl-main.203},
    doi = {10.18653/v1/2021.naacl-main.203},
    pages = {2579--2590},
    abstract = {In this work we explore Unsupervised Domain Adaptation (UDA) of pretrained language models for downstream tasks. We introduce UDALM, a fine-tuning procedure, using a mixed classification and Masked Language Model loss, that can adapt to the target domain distribution in a robust and sample efficient manner. Our experiments show that performance of models trained with the mixed loss scales with the amount of available target data and the mixed loss can be effectively used as a stopping criterion during UDA training. Furthermore, we discuss the relationship between A-distance and the target error and explore some limitations of the Domain Adversarial Training approach. Our method is evaluated on twelve domain pairs of the Amazon Reviews Sentiment dataset, yielding 91.74{\%} accuracy, which is an 1.11{\%} absolute improvement over the state-of-the-art.},
    arxiv = {2104.07078},
    video = {https://aclanthology.org/2021.naacl-main.203.mp4},
    code = {https://github.com/ckarouzos/slp_daptmlm},
    html = {https://aclanthology.org/2021.naacl-main.203/}
}

@article{chatziagapi-etal-2022-audio,
    title = {Audio and ASR-based Filled Pause Detection},
    selected={false},
    author = {Chatziagapi, Aggelina  and
      Sgouropoulos, Dimitris  and
      Karouzos, Constantinos and
      Melistas, Thomas and
      Giannakopoulos, Theodoros and
      Katsamanis, Athanasios and
      Narayanan, Shrikanth},
    booktitle = {Proceedings of 2022 10th International Conference on Affective Computing and Intelligent Interaction (ACII)},
    month = {10},
    year = {2022},
    address = {Nara, Japan},
    abbr = {ACII 2022},
    abstract = {Filled pauses (or fillers) are the most common form of speech disfluencies and they can be recognized as hesitation markers ("um", "uh" and "er") made by speakers, usually to gain extra time while thinking their next words. Filled pauses are very frequent in spontaneous speech. Their detection is therefore rather important for two basic reasons: (a) their existence influences the performance of individual components, like Automatic Speech Recognition system (ASR), in human-machine interaction and (b) their frequency can characterize the overall speech quality of a particular speaker, as it can be strongly associated with the speaker's confidence. Despite that, only limited work has been published for the detection of filled pauses in speech, especially through audio. In this work, we propose a framework for filled pause detection using both audio and textual information. For the audio modality, we transfer knowledge from a plethora of supervised tasks, such as emotion or speaking rate, using Convolutional Neural Networks (CNNs). For the text modality, we develop a temporal Recurrent Neural Network (RNN) method that takes into account textual information derived from an ASR system. In addition, the proposed transfer learning approach for the audio classifier leads to better results when benchmarked on our internal dataset for which the text is not transcribed but estimated by an ASR system. In this case, a simple late fusion approach boosts the performance even further. This proves that the audio approach is suitable for real-world applications where the transcribed text is not available and has to leverage imperfect ASR results, or even the absence of textual information (to reduce computational cost).},
    html = {https://ieeexplore.ieee.org/document/9953889}
}